{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "inzynierka_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn5wnnje505e",
        "outputId": "81a6fde7-1c39-484c-8538-c8a28e44d34a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9 MB 10.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3 MB 29.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 50.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 895 kB 37.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYHRvjRO6Pnm",
        "outputId": "0e96fc51-2edb-4cfd-a25c-b296042e7bce"
      },
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxzgXDpEj181",
        "outputId": "bb292909-1cef-4d3f-b00e-11db590f01ae"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
        "import numpy as np\n",
        "\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "text = [\"Good night honey ðŸ˜Š\", \"lmao xd ðŸ˜Š\"]\n",
        "#text = preprocess(text)\n",
        "\n",
        "# Pytorch\n",
        "roberta = AutoModel.from_pretrained(MODEL).to(device)\n",
        "modules = [roberta.embeddings, *roberta.encoder.layer[:10]] #do przetestowania rÃ³Å¼ne parametry\n",
        "for module in modules:\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = False\n",
        "#encoded_input = tokenizer(text, return_tensors='pt', padding=True)\n",
        "#features = model(**encoded_input)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXhmdf8GYito"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def preprocess(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"train_dataset.csv\")\n",
        "X = df[\"tweet_content\"].tolist()\n",
        "X = [preprocess(x) for x in X]\n",
        "y = (df[\"sarcasm_label\"] == 'sarcastic').tolist()\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLQ00Vvt167U"
      },
      "source": [
        "from torch.nn.functional import pad\n",
        "X_encoded  = tokenizer(X, return_tensors='pt', padding=True)['input_ids']"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeHLYYKLHtOI"
      },
      "source": [
        "X_train, X_cross, y_train, y_cross = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "y_train = torch.Tensor(y_train).type(torch.long)\n",
        "y_cross = torch.Tensor(y_cross).type(torch.long)\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVEpbnsGnBFr"
      },
      "source": [
        "#dobija do f_score = 0.4 gdy trenujemy roberte\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self ):\n",
        "        super().__init__()\n",
        "        self.roberta = roberta\n",
        "        self.lstm = nn.LSTM(768, 64, 2, bidirectional = True, dropout = 0.2)\n",
        "        self.linear1 = nn.Linear(716, 100)\n",
        "        self.linear2 = nn.Linear(100, 20)\n",
        "        self.linear3 = nn.Linear(20, 2)\n",
        "        self.pooling = nn.MaxPool2d((30, 5))\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p = 0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.roberta(x).last_hidden_state\n",
        "        x, _ =  self.lstm(x1)\n",
        "        x = torch.cat((x1, x), dim = 2)\n",
        "        x = self.pooling(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.linear1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear3(x)\n",
        "        return x"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-ksWbjx1zvR"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "dataset_train = TensorDataset(X_train, y_train) # create your datset\n",
        "dataloader_train = DataLoader(dataset_train,batch_size=40) # create your dataloader\n",
        "dataset_cross = TensorDataset(X_cross, y_cross) # create your datset\n",
        "dataloader_cross = DataLoader(dataset_cross,batch_size=40) # create your dataloader"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHvhH7x1smki"
      },
      "source": [
        "def train(dataloader, model, loss_fn, optimizer): #fragment z dokumentacji pytorch\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch % 50 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss :>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    train_loss /= num_batches\n",
        "    print(f\"Avg train loss: {train_loss:>8f} \\n\")\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbCbcRH531-o"
      },
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    true_positive, true_negative, false_negative, false_positive = 0.0001, 0.0001, 0.0001, 0.0001 #zeby dzielenia przez zero nie bylo\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            y_pred = pred.argmax(1)\n",
        "            correct += (y_pred == y).type(torch.long).sum().item()\n",
        "\n",
        "            true_positive += torch.logical_and(y_pred == y, y == 1).sum().item()\n",
        "            true_negative += torch.logical_and(y_pred == y, y == 0).sum().item()\n",
        "            false_negative += torch.logical_and(y_pred != y, y == 1).sum().item() \n",
        "            false_positive += torch.logical_and(y_pred != y, y == 0).sum().item()\n",
        "        \n",
        "\n",
        "    recall = true_positive/(true_positive + false_negative)\n",
        "    precision = true_positive/(true_positive + false_positive)\n",
        "    specificity = true_negative/(false_positive + true_negative)\n",
        "    f_score = (2*precision*recall)/(precision + recall)\n",
        "    g_mean = (recall*specificity)**(0.5)\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    print(f\"\\n recall: {recall} \\n precision: {precision} \\n specificity: {specificity} \\n f_score: {f_score} \\n g_mean: {g_mean}\\n\")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5ZkvpljVBoG"
      },
      "source": [
        "model = Model().to(device)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0MzXjEZ5VLE"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss(weight = torch.Tensor([1.0, 1.0 ])).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF1mZGWKxYJE",
        "outputId": "d79dd88d-d7a8-40fb-87f3-3c563531cdd6"
      },
      "source": [
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "    train(dataloader_train, model, loss_fn, optimizer)\n",
        "    test(dataloader_cross, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.305836  [    0/ 2260]\n",
            "loss: 0.369402  [ 2000/ 2260]\n",
            "Avg train loss: 0.319527 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 40.7%, Avg loss: 0.723447 \n",
            "\n",
            "\n",
            " recall: 0.7380946712031637 \n",
            " precision: 0.16533351182212705 \n",
            " specificity: 0.34927241194494313 \n",
            " f_score: 0.270152705749276 \n",
            " g_mean: 0.5077362563918779\n",
            "\n",
            "loss: 0.331267  [    0/ 2260]\n",
            "loss: 0.384424  [ 2000/ 2260]\n",
            "Avg train loss: 0.304318 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.2%, Avg loss: 0.766578 \n",
            "\n",
            "\n",
            " recall: 0.7738088718836383 \n",
            " precision: 0.16455713186980667 \n",
            " specificity: 0.31392939129755043 \n",
            " f_score: 0.27139893828898687 \n",
            " g_mean: 0.4928705186264185\n",
            "\n",
            "loss: 0.226590  [    0/ 2260]\n",
            "loss: 0.472294  [ 2000/ 2260]\n",
            "Avg train loss: 0.285544 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.5%, Avg loss: 0.815355 \n",
            "\n",
            "\n",
            " recall: 0.7857136054437965 \n",
            " precision: 0.1645886959657377 \n",
            " specificity: 0.3035343852247879 \n",
            " f_score: 0.2721651363586505 \n",
            " g_mean: 0.4883555018540636\n",
            "\n",
            "loss: 0.278900  [    0/ 2260]\n",
            "loss: 0.360671  [ 2000/ 2260]\n",
            "Avg train loss: 0.259243 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.4%, Avg loss: 0.880152 \n",
            "\n",
            "\n",
            " recall: 0.7976183390039547 \n",
            " precision: 0.16144594629111025 \n",
            " specificity: 0.2765073694356053 \n",
            " f_score: 0.2685372596895714 \n",
            " g_mean: 0.4696246892270256\n",
            "\n",
            "loss: 0.241408  [    0/ 2260]\n",
            "loss: 0.474977  [ 2000/ 2260]\n",
            "Avg train loss: 0.246742 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 0.908035 \n",
            "\n",
            "\n",
            " recall: 0.8095230725641129 \n",
            " precision: 0.1626795872346473 \n",
            " specificity: 0.27234936700650025 \n",
            " f_score: 0.2709165171979943 \n",
            " g_mean: 0.4695456275911781\n",
            "\n",
            "loss: 0.151483  [    0/ 2260]\n",
            "loss: 0.432143  [ 2000/ 2260]\n",
            "Avg train loss: 0.225759 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.6%, Avg loss: 0.936157 \n",
            "\n",
            "\n",
            " recall: 0.8095230725641129 \n",
            " precision: 0.16346170025879797 \n",
            " specificity: 0.2765073694356053 \n",
            " f_score: 0.27200018239985413 \n",
            " g_mean: 0.4731163654875315\n",
            "\n",
            "loss: 0.250267  [    0/ 2260]\n",
            "loss: 0.368536  [ 2000/ 2260]\n",
            "Avg train loss: 0.205982 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.2%, Avg loss: 1.016496 \n",
            "\n",
            "\n",
            " recall: 0.8452372732445875 \n",
            " precision: 0.16511643482956523 \n",
            " specificity: 0.25363835607552765 \n",
            " f_score: 0.27626476555271173 \n",
            " g_mean: 0.4630168382246144\n",
            "\n",
            "loss: 0.167048  [    0/ 2260]\n",
            "loss: 0.393582  [ 2000/ 2260]\n",
            "Avg train loss: 0.188899 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.064397 \n",
            "\n",
            "\n",
            " recall: 0.8333325396844293 \n",
            " precision: 0.16317032020031694 \n",
            " specificity: 0.25363835607552765 \n",
            " f_score: 0.27290466050318873 \n",
            " g_mean: 0.4597445980430907\n",
            "\n",
            "loss: 0.212092  [    0/ 2260]\n",
            "loss: 0.355357  [ 2000/ 2260]\n",
            "Avg train loss: 0.179052 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.9%, Avg loss: 1.136433 \n",
            "\n",
            "\n",
            " recall: 0.8690467403649038 \n",
            " precision: 0.1633111126124776 \n",
            " specificity: 0.22245333785724 \n",
            " f_score: 0.2749530885475793 \n",
            " g_mean: 0.43968437332719373\n",
            "\n",
            "loss: 0.123620  [    0/ 2260]\n",
            "loss: 0.389526  [ 2000/ 2260]\n",
            "Avg train loss: 0.177032 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.122253 \n",
            "\n",
            "\n",
            " recall: 0.8333325396844293 \n",
            " precision: 0.16166297382772574 \n",
            " specificity: 0.2453223512173176 \n",
            " f_score: 0.27079321408648815 \n",
            " g_mean: 0.452144996634136\n",
            "\n",
            "loss: 0.156468  [    0/ 2260]\n",
            "loss: 0.361520  [ 2000/ 2260]\n",
            "Avg train loss: 0.160513 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.9%, Avg loss: 1.189069 \n",
            "\n",
            "\n",
            " recall: 0.8690467403649038 \n",
            " precision: 0.16553303150429413 \n",
            " specificity: 0.23492734514455507 \n",
            " f_score: 0.2780954071654041 \n",
            " g_mean: 0.4518438264715545\n",
            "\n",
            "loss: 0.127466  [    0/ 2260]\n",
            "loss: 0.323068  [ 2000/ 2260]\n",
            "Avg train loss: 0.154023 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.240052 \n",
            "\n",
            "\n",
            " recall: 0.8690467403649038 \n",
            " precision: 0.1629465790417058 \n",
            " specificity: 0.2203743366426875 \n",
            " f_score: 0.2744362598223611 \n",
            " g_mean: 0.43762495235007515\n",
            "\n",
            "loss: 0.091774  [    0/ 2260]\n",
            "loss: 0.310115  [ 2000/ 2260]\n",
            "Avg train loss: 0.132951 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.6%, Avg loss: 1.307622 \n",
            "\n",
            "\n",
            " recall: 0.8690467403649038 \n",
            " precision: 0.16079310097220223 \n",
            " specificity: 0.20790032935537245 \n",
            " f_score: 0.2713756346649556 \n",
            " g_mean: 0.4250589412623576\n",
            "\n",
            "loss: 0.106263  [    0/ 2260]\n",
            "loss: 0.345877  [ 2000/ 2260]\n",
            "Avg train loss: 0.126284 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.2%, Avg loss: 1.335575 \n",
            "\n",
            "\n",
            " recall: 0.8690467403649038 \n",
            " precision: 0.16186267766621837 \n",
            " specificity: 0.21413733299902996 \n",
            " f_score: 0.2728973660580441 \n",
            " g_mean: 0.4313877040821179\n",
            "\n",
            "loss: 0.099687  [    0/ 2260]\n",
            "loss: 0.423768  [ 2000/ 2260]\n",
            "Avg train loss: 0.120867 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.6%, Avg loss: 1.405250 \n",
            "\n",
            "\n",
            " recall: 0.880951473925062 \n",
            " precision: 0.16228084987682026 \n",
            " specificity: 0.20582132814081994 \n",
            " f_score: 0.27407424142648784 \n",
            " g_mean: 0.42581522094785335\n",
            "\n",
            "loss: 0.074032  [    0/ 2260]\n",
            "loss: 0.253703  [ 2000/ 2260]\n",
            "Avg train loss: 0.107066 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 1.479327 \n",
            "\n",
            "\n",
            " recall: 0.8928562074852202 \n",
            " precision: 0.16304362476364143 \n",
            " specificity: 0.1995843244971624 \n",
            " f_score: 0.2757354590180449 \n",
            " g_mean: 0.422137540434437\n",
            "\n",
            "loss: 0.104172  [    0/ 2260]\n",
            "loss: 0.276180  [ 2000/ 2260]\n",
            "Avg train loss: 0.103895 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 29.6%, Avg loss: 1.542988 \n",
            "\n",
            "\n",
            " recall: 0.9047609410453784 \n",
            " precision: 0.16309027335181403 \n",
            " specificity: 0.18918931842439984 \n",
            " f_score: 0.27636379900814617 \n",
            " g_mean: 0.41372829945919065\n",
            "\n",
            "loss: 0.085662  [    0/ 2260]\n",
            "loss: 0.210706  [ 2000/ 2260]\n",
            "Avg train loss: 0.100543 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 28.5%, Avg loss: 1.606999 \n",
            "\n",
            "\n",
            " recall: 0.9047609410453784 \n",
            " precision: 0.1610170927893675 \n",
            " specificity: 0.1767153111370848 \n",
            " f_score: 0.2733814579989511 \n",
            " g_mean: 0.3998563632124862\n",
            "\n",
            "loss: 0.093302  [    0/ 2260]\n",
            "loss: 0.286387  [ 2000/ 2260]\n",
            "Avg train loss: 0.094400 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 27.6%, Avg loss: 1.661534 \n",
            "\n",
            "\n",
            " recall: 0.9166656746055366 \n",
            " precision: 0.1607517074105606 \n",
            " specificity: 0.16424130384976973 \n",
            " f_score: 0.27353479677811954 \n",
            " g_mean: 0.3880133574911334\n",
            "\n",
            "loss: 0.117041  [    0/ 2260]\n",
            "loss: 0.255770  [ 2000/ 2260]\n",
            "Avg train loss: 0.082069 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 27.6%, Avg loss: 1.717358 \n",
            "\n",
            "\n",
            " recall: 0.9166656746055366 \n",
            " precision: 0.1607517074105606 \n",
            " specificity: 0.16424130384976973 \n",
            " f_score: 0.27353479677811954 \n",
            " g_mean: 0.3880133574911334\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWXKx3gzwXU9"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": []
    }
  ]
}